# LLM RAG Auto Eval

This project explores how large language models can be used to automatically evaluate the quality of Retrieval-Augmented Generation (RAG) outputs.

The goal is to assess whether a generated answer is grounded in the retrieved context and to evaluate additional properties such as relevance, completeness, and tone. The project emphasizes reproducibility, transparency, and careful analysis of automated evaluation methods.

This repository contains:
- Benchmark integrations for groundedness and hallucination detection
- A structured LLM-based judge for RAG evaluation
- Experimental pipelines for robustness and bias analysis
- Tools for applying the judge to real RAG systems

This project is intended as both a research exploration and a practical engineering exercise.
