{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf0613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_MODEL: llama3:8b\n",
      "Processed path exists: True | ../data/processed/ragtruth_processed.jsonl\n",
      "Sample path exists: True | ../data/samples/ragtruth_sample.jsonl\n"
     ]
    }
   ],
   "source": [
    "# setup - imports, paths, and model config\n",
    "\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "LLM_MODEL = \"llama3:8b\"\n",
    "DATA_ROOT = Path(\"..\") / \"data\"\n",
    "PROCESSED_PATH = DATA_ROOT / \"processed\" / \"ragtruth_processed.jsonl\"\n",
    "SAMPLE_PATH = DATA_ROOT / \"samples\" / \"ragtruth_sample.jsonl\"\n",
    "\n",
    "print(\"LLM_MODEL:\", LLM_MODEL)\n",
    "print(\"Processed path exists:\", PROCESSED_PATH.exists(), \"|\", PROCESSED_PATH)\n",
    "print(\"Sample path exists:\", SAMPLE_PATH.exists(), \"|\", SAMPLE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd45721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " outputs found\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Basic file presence checks\n",
    "\n",
    "assert SAMPLE_PATH.exists(), \"Sample file not found. Make sure Phase 1 ran successfully.\"\n",
    "print(\" outputs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10cddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi (first 20 lines):\n",
      "Fri Dec 26 19:46:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:AC:00.0  On |                  Off |\n",
      "| 30%   41C    P8             29W /  300W |    1407MiB /  49140MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:CA:00.0 Off |                  Off |\n",
      "| 30%   42C    P8             29W /  300W |      18MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU awareness quick check (system-level)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "print(\"nvidia-smi (first 20 lines):\")\n",
    "try:\n",
    "    out = subprocess.check_output([\"bash\", \"-lc\", \"nvidia-smi | head -n 20\"], text=True)\n",
    "    print(out)\n",
    "except Exception as e:\n",
    "    print(\"Could not run nvidia-smi:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36143fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shell OK. If nvidia-smi printed, GPU is visible to the OS.\n"
     ]
    }
   ],
   "source": [
    "# Confirm we can run shell commands\n",
    "\n",
    "print(\"Shell OK. If nvidia-smi printed, GPU is visible to the OS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecc9a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://localhost:11434\n",
      "ollama list:\n",
      " NAME         ID              SIZE      MODIFIED   \n",
      "llama3:8b    365c0bd3c000    4.7 GB    7 days ago    \n",
      "\n",
      "Ollama HTTP reachable. Number of models visible: 1\n"
     ]
    }
   ],
   "source": [
    "# Check Ollama is reachable and model is available\n",
    "\n",
    "import subprocess\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "print(\"Ollama host:\", OLLAMA_HOST)\n",
    "\n",
    "try:\n",
    "    # This requires ollama CLI to be installed in the environment.\n",
    "    models = subprocess.check_output([\"bash\", \"-lc\", \"ollama list\"], text=True)\n",
    "    print(\"ollama list:\\n\", models)\n",
    "except Exception as e:\n",
    "    print(\"Could not run `ollama list` (CLI not available?). Error:\", e)\n",
    "\n",
    "# Also check the HTTP endpoint is reachable\n",
    "try:\n",
    "    tags = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=10).json()\n",
    "    print(\"Ollama HTTP reachable. Number of models visible:\", len(tags.get(\"models\", [])))\n",
    "except Exception as e:\n",
    "    print(\"Ollama HTTP not reachable. Start Ollama and retry. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e2b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP status: 200\n",
      "Ollama server is reachable.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Basic Ollama HTTP reachability\n",
    "\n",
    "resp = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=10)\n",
    "print(\"HTTP status:\", resp.status_code)\n",
    "assert resp.status_code == 200, \"Ollama HTTP server not reachable at localhost:11434\"\n",
    "print(\"Ollama server is reachable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a587012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded examples: 10\n",
      "Keys: ['example_id', 'task', 'question', 'context', 'answer', 'label', 'meta']\n",
      "Preview example_id: 0\n",
      "Question preview: Summarize the following news within 116 words:\n",
      "Context preview: Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her supposed death on March 31, 1945, the Bergen-Belsen concentration camp where she ha\n",
      "Answer preview: The Anne Frank House has revealed that Anne Frank and her older sister, Margot, likely died at least a month earlier than previously believed. The sisters, who were imprisoned in Nazi concentration ca\n",
      "Human label preview (if any): {'evident_conflict': 0, 'baseless_info': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load a small set of examples to judge\n",
    "\n",
    "def read_jsonl(path: Path, limit=None):\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "examples = read_jsonl(SAMPLE_PATH, limit=10)\n",
    "\n",
    "print(\"Loaded examples:\", len(examples))\n",
    "print(\"Keys:\", list(examples[0].keys()))\n",
    "print(\"Preview example_id:\", examples[0].get(\"example_id\"))\n",
    "print(\"Question preview:\", (examples[0].get(\"question\") or \"\")[:200])\n",
    "print(\"Context preview:\", (examples[0].get(\"context\") or \"\")[:200])\n",
    "print(\"Answer preview:\", (examples[0].get(\"answer\") or \"\")[:200])\n",
    "print(\"Human label preview (if any):\", examples[0].get(\"label\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "832632c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample schema OK for judge.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Example schema looks usable for judging\n",
    "\n",
    "req = [\"example_id\", \"question\", \"context\", \"answer\"]\n",
    "for k in req:\n",
    "    assert k in examples[0], f\"Missing key in sample: {k}\"\n",
    "print(\"Sample schema OK for judge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0ac82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge rubric ready. Length: 984 chars\n"
     ]
    }
   ],
   "source": [
    "# Define the judge rubric and prompt builder (faithfulness/groundedness)\n",
    "\n",
    "JUDGE_RUBRIC = \"\"\"\n",
    "You are evaluating a Retrieval-Augmented Generation (RAG) answer.\n",
    "\n",
    "Your ONLY source of truth is the provided CONTEXT.\n",
    "Do not use outside knowledge. If the context does not support a claim, treat it as unsupported.\n",
    "\n",
    "Classify the ANSWER into one of these labels:\n",
    "- supported: All claims in the answer are supported by the context.\n",
    "- partially_supported: Some claims are supported, but the answer also includes unsupported claims OR important details are missing.\n",
    "- unsupported: The answer is not supported by the context (the context does not contain the needed facts).\n",
    "- contradicted: The answer contradicts the context.\n",
    "\n",
    "Return a JSON object with these keys exactly:\n",
    "{\n",
    "  \"label\": one of [\"supported\",\"partially_supported\",\"unsupported\",\"contradicted\"],\n",
    "  \"confidence\": a number from 0.0 to 1.0,\n",
    "  \"evidence\": a short quote or pointer from the context that supports your decision (or \"\" if none),\n",
    "  \"notes\": a short explanation (1-3 sentences)\n",
    "}\n",
    "\n",
    "Return ONLY valid JSON. No extra text.\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_judge_prompt(question: str, context: str, answer: str) -> str:\n",
    "    question = question.strip()\n",
    "    context = context.strip()\n",
    "    answer = answer.strip()\n",
    "    prompt = f\"\"\"\n",
    "{JUDGE_RUBRIC}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "{answer}\n",
    "\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "print(\"Judge rubric ready. Length:\", len(JUDGE_RUBRIC), \"chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d8e171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt preview (first 800 chars):\n",
      "\n",
      "You are evaluating a Retrieval-Augmented Generation (RAG) answer.\n",
      "\n",
      "Your ONLY source of truth is the provided CONTEXT.\n",
      "Do not use outside knowledge. If the context does not support a claim, treat it as unsupported.\n",
      "\n",
      "Classify the ANSWER into one of these labels:\n",
      "- supported: All claims in the answer are supported by the context.\n",
      "- partially_supported: Some claims are supported, but the answer also includes unsupported claims OR important details are missing.\n",
      "- unsupported: The answer is not supported by the context (the context does not contain the needed facts).\n",
      "- contradicted: The answer contradicts the context.\n",
      "\n",
      "Return a JSON object with these keys exactly:\n",
      "{\n",
      "  \"label\": one of [\"supported\",\"partially_supported\",\"unsupported\",\"contradicted\"],\n",
      "  \"confidence\": a number from 0.0 to 1.0,\n",
      "  \"ev\n",
      "... (truncated)\n",
      "Prompt looks OK.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Build and preview a judge prompt\n",
    "\n",
    "p = build_judge_prompt(\n",
    "    question=examples[0][\"question\"],\n",
    "    context=examples[0][\"context\"],\n",
    "    answer=examples[0][\"answer\"],\n",
    ")\n",
    "print(\"Prompt preview (first 800 chars):\\n\")\n",
    "print(p[:800] + (\"\\n... (truncated)\" if len(p) > 800 else \"\"))\n",
    "assert \"Return ONLY valid JSON\" in p\n",
    "print(\"Prompt looks OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09e60796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama generate helper ready.\n"
     ]
    }
   ],
   "source": [
    "# Ollama generate call helper (non-streaming for easy parsing)\n",
    "\n",
    "def ollama_generate(prompt: str, model: str, temperature: float = 0.0, timeout_s: int = 120) -> str:\n",
    "    url = f\"{OLLAMA_HOST}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout_s)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"response\", \"\")\n",
    "\n",
    "print(\"Ollama generate helper ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bab1832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output:\n",
      " Here is the JSON:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"ok\": true\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Quick smoke test with a tiny prompt\n",
    "\n",
    "smoke = ollama_generate(\"Return JSON: {\\\"ok\\\": true}\", model=LLM_MODEL, temperature=0.0, timeout_s=120)\n",
    "print(\"Raw model output:\\n\", smoke[:300] + (\"...\" if len(smoke) > 300 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ead245ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge parsing helpers ready.\n"
     ]
    }
   ],
   "source": [
    "# Robust JSON extraction for judge output\n",
    "\n",
    "def extract_json_object(text: str):\n",
    "    \"\"\"\n",
    "    Tries to pull the first JSON object from the model output.\n",
    "    This makes the pipeline resilient if the model adds extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    # Fast path: pure JSON\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Try to find a JSON object substring\n",
    "    \n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON object found in model output.\")\n",
    "    candidate = m.group(0)\n",
    "\n",
    "    return json.loads(candidate)\n",
    "\n",
    "def judge_one(example: dict, model: str, temperature: float = 0.0):\n",
    "    prompt = build_judge_prompt(example[\"question\"], example[\"context\"], example[\"answer\"])\n",
    "    raw = ollama_generate(prompt, model=model, temperature=temperature, timeout_s=180)\n",
    "\n",
    "    parsed = extract_json_object(raw)\n",
    "\n",
    "    # light validation\n",
    "    label = parsed.get(\"label\")\n",
    "    conf = parsed.get(\"confidence\")\n",
    "    if label not in [\"supported\", \"partially_supported\", \"unsupported\", \"contradicted\"]:\n",
    "        raise ValueError(f\"Unexpected label: {label}\")\n",
    "    if not isinstance(conf, (int, float)):\n",
    "        raise ValueError(f\"Invalid confidence type: {type(conf)}\")\n",
    "\n",
    "    result = {\n",
    "        \"example_id\": example.get(\"example_id\"),\n",
    "        \"judge_model\": model,\n",
    "        \"judge_label\": label,\n",
    "        \"judge_confidence\": float(conf),\n",
    "        \"judge_evidence\": parsed.get(\"evidence\", \"\"),\n",
    "        \"judge_notes\": parsed.get(\"notes\", \"\"),\n",
    "        \"raw_judge_output\": raw,\n",
    "        \"human_label\": example.get(\"label\", None),\n",
    "    }\n",
    "    return result\n",
    "\n",
    "print(\"Judge parsing helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaf3da46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_id: 0\n",
      "judge_label: supported\n",
      "judge_confidence: 1.0\n",
      "judge_evidence: The Anne Frank House has revealed that Anne Frank and her older sister, Margot, likely died at least a month earlier than previously believed.\n",
      "judge_notes: \n",
      "human_label: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "Single-example judging works.\n"
     ]
    }
   ],
   "source": [
    "# Run judge on 1 example and print the result\n",
    "\n",
    "one = examples[0]\n",
    "res = judge_one(one, model=LLM_MODEL, temperature=0.0)\n",
    "\n",
    "print(\"example_id:\", res[\"example_id\"])\n",
    "print(\"judge_label:\", res[\"judge_label\"])\n",
    "print(\"judge_confidence:\", res[\"judge_confidence\"])\n",
    "print(\"judge_evidence:\", (res[\"judge_evidence\"][:200] + (\"...\" if len(res[\"judge_evidence\"]) > 200 else \"\")))\n",
    "print(\"judge_notes:\", res[\"judge_notes\"])\n",
    "print(\"human_label:\", res[\"human_label\"])\n",
    "\n",
    "assert res[\"judge_label\"] in [\"supported\", \"partially_supported\", \"unsupported\", \"contradicted\"]\n",
    "assert 0.0 <= res[\"judge_confidence\"] <= 1.0\n",
    "print(\"Single-example judging works.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78e3b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK | 0 | judge=supported | human={'evident_conflict': 0, 'baseless_info': 0} | conf=1.00\n",
      "OK | 1 | judge=partially_supported | human={'evident_conflict': 0, 'baseless_info': 0} | conf=0.80\n",
      "OK | 2 | judge=supported | human={'evident_conflict': 1, 'baseless_info': 1} | conf=1.00\n",
      "FAIL | 3 | error=Expecting ',' delimiter: line 4 column 18 (char 74)\n",
      "FAIL | 4 | error=Expecting ',' delimiter: line 4 column 17 (char 73)\n",
      "FAIL | 5 | error=Expecting ',' delimiter: line 4 column 17 (char 73)\n",
      "OK | 6 | judge=supported | human={'evident_conflict': 0, 'baseless_info': 0} | conf=1.00\n",
      "OK | 7 | judge=supported | human={'evident_conflict': 0, 'baseless_info': 0} | conf=1.00\n",
      "OK | 8 | judge=supported | human={'evident_conflict': 0, 'baseless_info': 0} | conf=0.90\n",
      "FAIL | 9 | error=No JSON object found in model output.\n",
      "\n",
      "Saved judge outputs: ../results/runs/rag_judge_faithfulness/phase2_judge_outputs_sample.jsonl\n",
      "Rows saved: 6\n"
     ]
    }
   ],
   "source": [
    "# Batch judge a small set of examples and save results\n",
    "\n",
    "OUT_DIR = Path(\"..\") / \"results\" / \"runs\" / \"rag_judge_faithfulness\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "JUDGE_OUT_PATH = OUT_DIR / \"phase2_judge_outputs_sample.jsonl\"\n",
    "\n",
    "batch_results = []\n",
    "for ex in examples:\n",
    "    try:\n",
    "        r = judge_one(ex, model=LLM_MODEL, temperature=0.0)\n",
    "        batch_results.append(r)\n",
    "        print( \n",
    "            f\"OK | {r['example_id']} | \"\n",
    "        f\"judge={r['judge_label']} | \"\n",
    "        f\"human={r['human_label']} | \"\n",
    "        f\"conf={r['judge_confidence']:.2f}\"\n",
    ")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAIL | {ex.get('example_id')} | error={e}\")\n",
    "\n",
    "with JUDGE_OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in batch_results:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved judge outputs: {JUDGE_OUT_PATH}\")\n",
    "print(\"Rows saved:\", len(batch_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad125a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read back rows: 6\n",
      "Label counts: {'supported': 5, 'partially_supported': 1}\n",
      "sample run looks good.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Read back judge results and summarize label counts\n",
    "\n",
    "def count_labels(rows):\n",
    "    counts = {}\n",
    "    for r in rows:\n",
    "        lab = r.get(\"judge_label\", \"missing\")\n",
    "        counts[lab] = counts.get(lab, 0) + 1\n",
    "    return counts\n",
    "\n",
    "rows_back = read_jsonl(JUDGE_OUT_PATH, limit=None)\n",
    "print(\"Read back rows:\", len(rows_back))\n",
    "\n",
    "counts = count_labels(rows_back)\n",
    "print(\"Label counts:\", counts)\n",
    "\n",
    "assert len(rows_back) > 0, \"No judge outputs saved. Check Ollama and prompts.\"\n",
    "print(\"sample run looks good.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
