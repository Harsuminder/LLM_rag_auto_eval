{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf0613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_MODEL: llama3:8b\n",
      "Processed path exists: True | ../data/processed/ragtruth_processed.jsonl\n",
      "Sample path exists: True | ../data/samples/ragtruth_sample.jsonl\n"
     ]
    }
   ],
   "source": [
    "# setup - imports, paths, and model config\n",
    "\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "LLM_MODEL = \"llama3:8b\"\n",
    "DATA_ROOT = Path(\"..\") / \"data\"\n",
    "PROCESSED_PATH = DATA_ROOT / \"processed\" / \"ragtruth_processed.jsonl\"\n",
    "SAMPLE_PATH = DATA_ROOT / \"samples\" / \"ragtruth_sample.jsonl\"\n",
    "\n",
    "print(\"LLM_MODEL:\", LLM_MODEL)\n",
    "print(\"Processed path exists:\", PROCESSED_PATH.exists(), \"|\", PROCESSED_PATH)\n",
    "print(\"Sample path exists:\", SAMPLE_PATH.exists(), \"|\", SAMPLE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd45721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " outputs found.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Basic file presence checks\n",
    "\n",
    "assert SAMPLE_PATH.exists(), \"Sample file not found. Make sure Phase 1 ran successfully.\"\n",
    "print(\" outputs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d10cddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-smi (first 20 lines):\n",
      "Mon Dec 29 14:16:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:AC:00.0  On |                  Off |\n",
      "| 30%   42C    P8             29W /  300W |     831MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000 Ada Gene...    Off |   00000000:CA:00.0 Off |                  Off |\n",
      "| 30%   43C    P8             29W /  300W |      16MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU awareness quick check (system-level)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "print(\"nvidia-smi (first 20 lines):\")\n",
    "try:\n",
    "    out = subprocess.check_output([\"bash\", \"-lc\", \"nvidia-smi | head -n 20\"], text=True)\n",
    "    print(out)\n",
    "except Exception as e:\n",
    "    print(\"Could not run nvidia-smi:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36143fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shell OK. If nvidia-smi printed, GPU is visible to the OS.\n"
     ]
    }
   ],
   "source": [
    "# Confirm we can run shell commands\n",
    "\n",
    "print(\"Shell OK. If nvidia-smi printed, GPU is visible to the OS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecc9a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama host: http://localhost:11434\n",
      "ollama list:\n",
      " NAME         ID              SIZE      MODIFIED   \n",
      "llama3:8b    365c0bd3c000    4.7 GB    9 days ago    \n",
      "\n",
      "Ollama HTTP reachable. Number of models visible: 1\n"
     ]
    }
   ],
   "source": [
    "# Check Ollama is reachable and model is available\n",
    "\n",
    "import subprocess\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "print(\"Ollama host:\", OLLAMA_HOST)\n",
    "\n",
    "try:\n",
    "    # This requires ollama CLI to be installed in the environment.\n",
    "    models = subprocess.check_output([\"bash\", \"-lc\", \"ollama list\"], text=True)\n",
    "    print(\"ollama list:\\n\", models)\n",
    "except Exception as e:\n",
    "    print(\"Could not run `ollama list` (CLI not available?). Error:\", e)\n",
    "\n",
    "# Also check the HTTP endpoint is reachable\n",
    "try:\n",
    "    tags = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=10).json()\n",
    "    print(\"Ollama HTTP reachable. Number of models visible:\", len(tags.get(\"models\", [])))\n",
    "except Exception as e:\n",
    "    print(\"Ollama HTTP not reachable. Start Ollama and retry. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44e2b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP status: 200\n",
      "Ollama server is reachable.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Basic Ollama HTTP reachability\n",
    "\n",
    "resp = requests.get(f\"{OLLAMA_HOST}/api/tags\", timeout=10)\n",
    "print(\"HTTP status:\", resp.status_code)\n",
    "assert resp.status_code == 200, \"Ollama HTTP server not reachable at localhost:11434\"\n",
    "print(\"Ollama server is reachable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a587012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded examples: 200\n",
      "Keys: ['example_id', 'task', 'question', 'context', 'answer', 'label', 'meta']\n",
      "Preview example_id: 0\n",
      "Question preview: Summarize the following news within 116 words:\n",
      "Context preview: Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her supposed death on March 31, 1945, the Bergen-Belsen concentration camp where she ha\n",
      "Answer preview: The Anne Frank House has revealed that Anne Frank and her older sister, Margot, likely died at least a month earlier than previously believed. The sisters, who were imprisoned in Nazi concentration ca\n",
      "Human label preview (if any): {'evident_conflict': 0, 'baseless_info': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load a small set of examples to judge\n",
    "\n",
    "def read_jsonl(path: Path, limit=None):\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit is not None and i >= limit:\n",
    "                break\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "# examples = read_jsonl(SAMPLE_PATH, limit=10)\n",
    "examples = read_jsonl(PROCESSED_PATH, limit=200)\n",
    "\n",
    "print(\"Loaded examples:\", len(examples))\n",
    "print(\"Keys:\", list(examples[0].keys()))\n",
    "print(\"Preview example_id:\", examples[0].get(\"example_id\"))\n",
    "print(\"Question preview:\", (examples[0].get(\"question\") or \"\")[:200])\n",
    "print(\"Context preview:\", (examples[0].get(\"context\") or \"\")[:200])\n",
    "print(\"Answer preview:\", (examples[0].get(\"answer\") or \"\")[:200])\n",
    "print(\"Human label preview (if any):\", examples[0].get(\"label\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "832632c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample schema OK for judge.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Example schema looks usable for judging\n",
    "\n",
    "req = [\"example_id\", \"question\", \"context\", \"answer\"]\n",
    "for k in req:\n",
    "    assert k in examples[0], f\"Missing key in sample: {k}\"\n",
    "print(\"Sample schema OK for judge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8273e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge rubric ready. Length: 1053 chars\n"
     ]
    }
   ],
   "source": [
    "# Define the judge rubric and prompt builder (faithfulness/groundedness)\n",
    "\n",
    "JUDGE_RUBRIC = \"\"\"\n",
    "You are evaluating a Retrieval-Augmented Generation (RAG) answer.\n",
    "\n",
    "Your ONLY source of truth is the provided CONTEXT.\n",
    "Do not use outside knowledge.\n",
    "\n",
    "Your task is to determine whether the ANSWER is fully supported by the CONTEXT.\n",
    "\n",
    "Decision rules:\n",
    "- PASS: All factual claims in the answer are directly supported by the context.\n",
    "- FAIL: Any claim is unsupported, missing from the context, or contradicted by the context.\n",
    "\n",
    "Then:\n",
    "1. Give a short reason (1–2 sentences).\n",
    "2. Provide a numeric uncertainty score as negative log likelihood (NLL).\n",
    "   - Lower NLL = more confident\n",
    "   - Higher NLL = less confident\n",
    "\n",
    "Use the following format EXACTLY:\n",
    "\n",
    "EVALUATION: PASS or FAIL\n",
    "REASON: <short explanation>\n",
    "NLL: <floating point number>\n",
    "\n",
    "Examples:\n",
    "\n",
    "EVALUATION: PASS\n",
    "REASON: The answer restates facts that are explicitly mentioned in the provided context.\n",
    "NLL: 0.12\n",
    "\n",
    "EVALUATION: FAIL\n",
    "REASON: The answer introduces details that do not appear in the provided context.\n",
    "NLL: 1.05\n",
    "\n",
    "Return ONLY the three lines above.\n",
    "Do not include JSON, markdown, code blocks, or extra text.\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_judge_prompt(question: str, context: str, answer: str) -> str:\n",
    "    question = question.strip()\n",
    "    context = context.strip()\n",
    "    answer = answer.strip()\n",
    "    prompt = f\"\"\"\n",
    "{JUDGE_RUBRIC}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "{answer}\n",
    "\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "print(\"Judge rubric ready. Length:\", len(JUDGE_RUBRIC), \"chars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d8e171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt preview (first 800 chars):\n",
      "\n",
      "You are evaluating a Retrieval-Augmented Generation (RAG) answer.\n",
      "\n",
      "The provided CONTEXT is the ONLY source of truth.\n",
      "Do NOT use outside knowledge.\n",
      "Do NOT assume missing facts are true.\n",
      "\n",
      "IMPORTANT DECISION RULE:\n",
      "- The answer may contain multiple factual claims.\n",
      "- You must evaluate EACH factual claim independently.\n",
      "- If ANY factual claim is not explicitly supported by the context, the final evaluation MUST be FAIL.\n",
      "\n",
      "Evaluation steps you must follow internally:\n",
      "1. Identify all factual claims in the answer.\n",
      "2. For each claim, check whether it is directly supported by the context.\n",
      "3. If even one claim is unsupported, incomplete, or contradicted, output FAIL.\n",
      "\n",
      "Output format (exactly):\n",
      "\n",
      "EVALUATION: PASS or FAIL\n",
      "REASON: One short sentence explaining the decision.\n",
      "NLL: A numeric uncertainty value (\n",
      "... (truncated)\n",
      "Prompt looks OK.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Build and preview a judge prompt\n",
    "\n",
    "p = build_judge_prompt(\n",
    "    question=examples[0][\"question\"],\n",
    "    context=examples[0][\"context\"],\n",
    "    answer=examples[0][\"answer\"],\n",
    ")\n",
    "\n",
    "print(\"Prompt preview (first 800 chars):\\n\")\n",
    "print(p[:800] + (\"\\n... (truncated)\" if len(p) > 800 else \"\"))\n",
    "\n",
    "assert \"EVALUATION:\" in p\n",
    "assert \"REASON:\" in p\n",
    "assert \"NLL:\" in p\n",
    "\n",
    "print(\"Prompt looks OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09e60796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama generate call helper (non-streaming for easy parsing)\n",
    "\n",
    "def ollama_generate(prompt: str, model: str, temperature: float = 0.0, timeout_s: int = 120) -> str:\n",
    "    url = f\"{OLLAMA_HOST}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"stop\": [\"\\n\\n\"],  # stop after first block\n",
    "        },\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout_s)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"response\", \"\").strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bab1832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output:\n",
      " EVALUATION: PASS\n",
      "REASON: The answer matches the context, which provides the correct solution to the arithmetic problem.\n",
      "NLL: 0.0\n",
      "Smoke test passed: judge-style output produced.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Quick smoke test using judge-style prompt\n",
    "\n",
    "dummy_prompt = \"\"\"\n",
    "You are evaluating a Retrieval-Augmented Generation (RAG) answer.\n",
    "\n",
    "Your ONLY source of truth is the CONTEXT.\n",
    "\n",
    "QUESTION:\n",
    "What is 2 + 2?\n",
    "\n",
    "CONTEXT:\n",
    "2 + 2 equals 4.\n",
    "\n",
    "ANSWER:\n",
    "2 + 2 is 4.\n",
    "\n",
    "Respond in the following format:\n",
    "\n",
    "EVALUATION: PASS or FAIL\n",
    "REASON: <short explanation>\n",
    "NLL: <number>\n",
    "\"\"\"\n",
    "\n",
    "smoke = ollama_generate(\n",
    "    dummy_prompt,\n",
    "    model=LLM_MODEL,\n",
    "    temperature=0.0,\n",
    "    timeout_s=120,\n",
    ")\n",
    "\n",
    "print(\"Raw model output:\\n\", smoke)\n",
    "\n",
    "assert \"EVALUATION:\" in smoke, \"Missing EVALUATION field\"\n",
    "assert \"REASON:\" in smoke, \"Missing REASON field\"\n",
    "assert \"NLL:\" in smoke, \"Missing NLL field\"\n",
    "\n",
    "print(\"Smoke test passed: judge-style output produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ead245ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust text extraction for judge output (PASS / FAIL format)\n",
    "\n",
    "def extract_judge_fields(text: str):\n",
    "    \"\"\"\n",
    "    Extracts evaluation, reason, and NLL from judge output.\n",
    "    Expected format:\n",
    "\n",
    "    EVALUATION: PASS or FAIL\n",
    "    REASON: <text>\n",
    "    NLL: <float>\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    eval_match = re.search(r\"EVALUATION:\\s*(PASS|FAIL)\", text)\n",
    "    reason_match = re.search(r\"REASON:\\s*(.+)\", text)\n",
    "    nll_match = re.search(r\"NLL:\\s*([0-9]*\\.?[0-9]+)\", text)\n",
    "\n",
    "    if not eval_match or not reason_match or not nll_match:\n",
    "        raise ValueError(\"Failed to extract required judge fields.\")\n",
    "\n",
    "    return {\n",
    "        \"evaluation\": eval_match.group(1),\n",
    "        \"reason\": reason_match.group(1).strip(),\n",
    "        \"nll\": float(nll_match.group(1)),\n",
    "    }\n",
    "\n",
    "\n",
    "def judge_one(example: dict, model: str, temperature: float = 0.0):\n",
    "    prompt = build_judge_prompt(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        example[\"answer\"],\n",
    "    )\n",
    "\n",
    "    raw = ollama_generate(\n",
    "        prompt,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        timeout_s=180,\n",
    "    )\n",
    "\n",
    "    parsed = extract_judge_fields(raw)\n",
    "\n",
    "    # light validation\n",
    "    if parsed[\"evaluation\"] not in [\"PASS\", \"FAIL\"]:\n",
    "        raise ValueError(f\"Unexpected evaluation: {parsed['evaluation']}\")\n",
    "    if not isinstance(parsed[\"nll\"], float):\n",
    "        raise ValueError(\"NLL must be a float.\")\n",
    "\n",
    "    result = {\n",
    "        \"example_id\": example.get(\"example_id\"),\n",
    "        \"judge_model\": model,\n",
    "        \"evaluation\": parsed[\"evaluation\"],\n",
    "        \"nll\": parsed[\"nll\"],\n",
    "        \"reason\": parsed[\"reason\"],\n",
    "        \"raw_judge_output\": raw,\n",
    "        \"human_label\": example.get(\"label\", None),\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf3da46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_id: 0\n",
      "evaluation: PASS\n",
      "nll: 0.9\n",
      "reason: Every factual claim in the answer is fully supported by the context.\n",
      "human_label: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "Single-example judging works.\n"
     ]
    }
   ],
   "source": [
    "# Run judge on 1 example and print the result\n",
    "\n",
    "one = examples[0]\n",
    "res = judge_one(one, model=LLM_MODEL, temperature=0.0)\n",
    "\n",
    "print(\"example_id:\", res[\"example_id\"])\n",
    "print(\"evaluation:\", res[\"evaluation\"])\n",
    "print(\"nll:\", res[\"nll\"])\n",
    "print(\"reason:\", res[\"reason\"])\n",
    "print(\"human_label:\", res[\"human_label\"])\n",
    "\n",
    "assert res[\"evaluation\"] in [\"PASS\", \"FAIL\"]\n",
    "assert isinstance(res[\"nll\"], float)\n",
    "print(\"Single-example judging works.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK | 0 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 1 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 2 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 1} | nll=0.010\n",
      "OK | 3 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.120\n",
      "OK | 4 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 5 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 1} | nll=0.010\n",
      "OK | 6 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 7 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 8 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 9 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 10 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.050\n",
      "OK | 11 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 12 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 13 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 14 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 15 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.150\n",
      "OK | 16 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 17 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 18 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 19 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 20 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 21 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.010\n",
      "OK | 22 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.010\n",
      "OK | 23 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.010\n",
      "OK | 30 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 31 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.020\n",
      "OK | 32 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 33 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.050\n",
      "OK | 34 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 35 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 36 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 37 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 38 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 39 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.120\n",
      "OK | 40 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.010\n",
      "OK | 41 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 48 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.020\n",
      "OK | 49 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 50 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 51 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.120\n",
      "OK | 52 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 53 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.020\n",
      "OK | 60 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 61 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 62 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 63 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 64 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 1} | nll=0.010\n",
      "OK | 65 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 72 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.020\n",
      "OK | 73 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.020\n",
      "OK | 74 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 75 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.010\n",
      "OK | 76 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 77 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 78 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 79 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.050\n",
      "OK | 80 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.000\n",
      "OK | 81 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 82 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.010\n",
      "OK | 83 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.010\n",
      "OK | 84 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 85 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 86 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.010\n",
      "OK | 87 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.010\n",
      "OK | 88 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 1} | nll=0.010\n",
      "OK | 89 | evaluation=FAIL | human={'evident_conflict': 0, 'baseless_info': 1} | nll=1.050\n",
      "OK | 90 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 91 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 1} | nll=0.120\n",
      "OK | 92 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 93 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.120\n",
      "OK | 94 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.050\n",
      "OK | 95 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 96 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 97 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 98 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.010\n",
      "OK | 99 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 100 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 101 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 102 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 103 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 104 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 105 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.050\n",
      "OK | 106 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.050\n",
      "OK | 107 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 108 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.120\n",
      "OK | 109 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 110 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 1} | nll=0.120\n",
      "OK | 111 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 1} | nll=0.050\n",
      "OK | 112 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.010\n",
      "OK | 113 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.010\n",
      "OK | 114 | evaluation=PASS | human={'evident_conflict': 1, 'baseless_info': 0} | nll=0.120\n",
      "OK | 115 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n",
      "OK | 116 | evaluation=PASS | human={'evident_conflict': 0, 'baseless_info': 0} | nll=0.120\n"
     ]
    }
   ],
   "source": [
    "# Batch judge a small set of examples and save results\n",
    "\n",
    "OUT_DIR = Path(\"..\") / \"results\" / \"runs\" / \"rag_judge_faithfulness\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "JUDGE_OUT_PATH = OUT_DIR / \"phase2_judge_outputs_200.jsonl\"\n",
    "\n",
    "batch_results = []\n",
    "\n",
    "for ex in examples:\n",
    "    try:\n",
    "        r = judge_one(ex, model=LLM_MODEL, temperature=0.0)\n",
    "        batch_results.append(r)\n",
    "\n",
    "        print(\n",
    "            f\"OK | {r['example_id']} | \"\n",
    "            f\"evaluation={r['evaluation']} | \"\n",
    "            f\"human={r['human_label']} | \"\n",
    "            f\"nll={r['nll']:.3f}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAIL | {ex.get('example_id')} | error={e}\")\n",
    "\n",
    "with JUDGE_OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in batch_results:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved judge outputs: {JUDGE_OUT_PATH}\")\n",
    "print(\"Rows saved:\", len(batch_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad125a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read back rows: 10\n",
      "Evaluation counts: {'PASS': 10}\n",
      "Sample run looks good.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Read back judge results and summarize evaluation counts\n",
    "\n",
    "def count_evaluations(rows):\n",
    "    counts = {}\n",
    "    for r in rows:\n",
    "        ev = r.get(\"evaluation\", \"missing\")\n",
    "        counts[ev] = counts.get(ev, 0) + 1\n",
    "    return counts\n",
    "\n",
    "rows_back = read_jsonl(JUDGE_OUT_PATH, limit=None)\n",
    "print(\"Read back rows:\", len(rows_back))\n",
    "\n",
    "counts = count_evaluations(rows_back)\n",
    "print(\"Evaluation counts:\", counts)\n",
    "\n",
    "assert len(rows_back) > 0, \"No judge outputs saved. Check Ollama and prompts.\"\n",
    "assert \"PASS\" in counts or \"FAIL\" in counts, \"No valid evaluations found.\"\n",
    "print(\"Sample run looks good.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51730ea",
   "metadata": {},
   "source": [
    "## Note: Why the Judge Output Format Was Changed\n",
    "\n",
    "Originally, the LLM judge was asked to return a strict JSON object (label, confidence, evidence, notes).  \n",
    "In practice, this caused frequent failures because the model often produced slightly invalid JSON\n",
    "(extra text, missing commas, formatting issues), especially when running batch evaluations with a local model.\n",
    "\n",
    "To make the pipeline more robust, the judge output contract was changed.\n",
    "\n",
    "Instead of JSON, the judge now returns structured plain text with three fields:\n",
    "- **EVALUATION**: PASS or FAIL (whether the answer is supported by the retrieved context)\n",
    "- **REASON**: a short explanation of the decision\n",
    "- **NLL**: a numeric uncertainty signal (negative log likelihood)\n",
    "\n",
    "This text output is easier for the model to produce reliably.  \n",
    "Regex is then used to extract these fields, and the results are stored as JSON afterward.\n",
    "\n",
    "This change improves stability without changing the judge’s reasoning logic.\n",
    "It separates *generation robustness* (LLM output) from *data structure* (storage and evaluation).\n",
    "\n",
    "In short:\n",
    "- JSON generation was fragile\n",
    "- Text + regex is more reliable\n",
    "- Structured data is still preserved after parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5565a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
