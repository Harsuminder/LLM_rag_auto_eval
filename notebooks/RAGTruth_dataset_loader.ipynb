{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc28a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX 6000 Ada Generation\n",
      "Using device: cuda:0\n",
      "GPU matmul result (mean): -0.00133326998911798\n"
     ]
    }
   ],
   "source": [
    "# Environment and GPU check (Torch)\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Do a tiny GPU computation to confirm the notebook is actually using the GPU\n",
    "a = torch.randn(1024, 1024, device=device)\n",
    "b = torch.randn(1024, 1024, device=device)\n",
    "c = a @ b\n",
    "print(\"GPU matmul result (mean):\", c.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318895c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor device check: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: GPU tensor location\n",
    "\n",
    "print(\"Tensor device check:\", c.device)\n",
    "assert str(c.device).startswith(\"cuda\") or device.type == \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31619ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed/verified: datasets, tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install/verify required packages (datasets, tqdm)\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q datasets tqdm\n",
    "print(\"Installed/verified: datasets, tqdm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c8f629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets.load_dataset available: True\n",
      "tqdm imported: True\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Import check\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"datasets.load_dataset available:\", callable(load_dataset))\n",
    "print(\"tqdm imported:\", tqdm is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25eb390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_REPO: wandb/RAGTruth-processed\n",
      "SPLIT: train\n",
      "RAW_OUT: ../data/raw/ragtruth_raw.jsonl\n",
      "PROCESSED_OUT: ../data/processed/ragtruth_processed.jsonl\n",
      "SAMPLE_OUT: ../data/samples/ragtruth_sample.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Configure dataset source and output paths\n",
    "\n",
    "HF_REPO = \"wandb/RAGTruth-processed\"   # change if you use a different RAGTruth variant\n",
    "SPLIT = \"train\"                       # change if needed\n",
    "LIMIT = None                          # set to an integer (e.g., 2000) for quick iteration\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "SAMPLES_DIR = DATA_DIR / \"samples\"\n",
    "\n",
    "RAW_OUT = RAW_DIR / \"ragtruth_raw.jsonl\"\n",
    "PROCESSED_OUT = PROCESSED_DIR / \"ragtruth_processed.jsonl\"\n",
    "SAMPLE_OUT = SAMPLES_DIR / \"ragtruth_sample.jsonl\"\n",
    "SAMPLE_N = 50\n",
    "\n",
    "for p in [RAW_DIR, PROCESSED_DIR, SAMPLES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"HF_REPO:\", HF_REPO)\n",
    "print(\"SPLIT:\", SPLIT)\n",
    "print(\"RAW_OUT:\", RAW_OUT)\n",
    "print(\"PROCESSED_OUT:\", PROCESSED_OUT)\n",
    "print(\"SAMPLE_OUT:\", SAMPLE_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca0b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All output directories exist.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Paths exist\n",
    "\n",
    "assert RAW_DIR.exists()\n",
    "assert PROCESSED_DIR.exists()\n",
    "assert SAMPLES_DIR.exists()\n",
    "print(\"All output directories exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc1eeb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: wandb/RAGTruth-processed | split: train\n",
      "Number of rows: 15090\n",
      "First row keys: ['id', 'query', 'context', 'output', 'task_type', 'quality', 'model', 'temperature', 'hallucination_labels', 'hallucination_labels_processed', 'input_str']\n",
      "- id: 0\n",
      "- query: Summarize the following news within 116 words:\n",
      "- context: Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her supposed death on March 31, 1945, the Bergen-Belsen concentration camp where she ha ... (truncated)\n",
      "- output: The Anne Frank House has revealed that Anne Frank and her older sister, Margot, likely died at least a month earlier than previously believed. The sisters, who were imprisoned in Nazi concentration ca ... (truncated)\n",
      "- task_type: Summary\n",
      "- quality: good\n",
      "- model: gpt-4-0613\n",
      "- temperature: 0.699999988079071\n",
      "- hallucination_labels: []\n",
      "- hallucination_labels_processed: {'evident_conflict': 0, 'baseless_info': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from Hugging Face and inspect columns\n",
    "\n",
    "ds = load_dataset(HF_REPO, split=SPLIT)\n",
    "print(\"Loaded dataset:\", HF_REPO, \"| split:\", SPLIT)\n",
    "print(\"Number of rows:\", len(ds))\n",
    "\n",
    "first_row = ds[0]\n",
    "print(\"First row keys:\", list(first_row.keys()))\n",
    "\n",
    "# Print a small preview (truncate long fields)\n",
    "for k in list(first_row.keys())[:10]:\n",
    "    v = first_row[k]\n",
    "    s = str(v)\n",
    "    if len(s) > 200:\n",
    "        s = s[:200] + \" ... (truncated)\"\n",
    "    print(f\"- {k}: {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abbcdc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset looks non-empty and readable.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Dataset non-empty\n",
    "\n",
    "assert len(ds) > 0\n",
    "print(\"Dataset looks non-empty and readable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2e36f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving raw JSONL: 100%|██████████| 15090/15090 [00:01<00:00, 15008.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw JSONL: ../data/raw/ragtruth_raw.jsonl (15090 rows)\n"
     ]
    }
   ],
   "source": [
    "# Save raw dataset snapshot to data/raw (JSONL)\n",
    "\n",
    "def write_jsonl(path: Path, rows):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "raw_rows = []\n",
    "for i, row in enumerate(tqdm(ds, desc=\"Saving raw JSONL\")):\n",
    "    if LIMIT is not None and i >= LIMIT:\n",
    "        break\n",
    "    raw_rows.append(dict(row))\n",
    "\n",
    "write_jsonl(RAW_OUT, raw_rows)\n",
    "print(f\"Saved raw JSONL: {RAW_OUT} ({len(raw_rows)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1497e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw file size (bytes): 115342141\n",
      "First raw JSONL line (truncated): {\"id\": \"0\", \"query\": \"Summarize the following news within 116 words:\", \"context\": \"Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her ...\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Raw file exists and has content\n",
    "\n",
    "assert RAW_OUT.exists()\n",
    "raw_size = RAW_OUT.stat().st_size\n",
    "print(\"Raw file size (bytes):\", raw_size)\n",
    "assert raw_size > 0\n",
    "\n",
    "# Print first line\n",
    "with RAW_OUT.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    line = f.readline().strip()\n",
    "print(\"First raw JSONL line (truncated):\", line[:200] + (\"...\" if len(line) > 200 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2eb4829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization helpers defined.\n"
     ]
    }
   ],
   "source": [
    "# Define normalization helpers (stable schema for RAG judge)\n",
    "\n",
    "def safe_str(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return x\n",
    "    return str(x)\n",
    "\n",
    "def pick_first_present(row, keys):\n",
    "    for k in keys:\n",
    "        if k in row and row[k] not in (None, \"\"):\n",
    "            return row[k]\n",
    "    return None\n",
    "\n",
    "def join_context(ctx):\n",
    "    # Normalize context to a single string\n",
    "    if ctx is None:\n",
    "        return \"\"\n",
    "    if isinstance(ctx, str):\n",
    "        return ctx.strip()\n",
    "\n",
    "    if isinstance(ctx, list):\n",
    "        parts = []\n",
    "        for item in ctx:\n",
    "            if item is None:\n",
    "                continue\n",
    "            if isinstance(item, str):\n",
    "                parts.append(item.strip())\n",
    "            elif isinstance(item, dict):\n",
    "                # common text-like keys\n",
    "                for key in (\"text\", \"content\", \"passage\", \"chunk\"):\n",
    "                    if key in item and item[key]:\n",
    "                        parts.append(safe_str(item[key]).strip())\n",
    "                        break\n",
    "                else:\n",
    "                    parts.append(safe_str(item).strip())\n",
    "            else:\n",
    "                parts.append(safe_str(item).strip())\n",
    "        parts = [p for p in parts if p]\n",
    "        return \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "    return safe_str(ctx).strip()\n",
    "\n",
    "def normalize_row(row, idx):\n",
    "    # ID\n",
    "    rid = pick_first_present(row, (\"id\", \"example_id\", \"uid\", \"qid\"))\n",
    "    example_id = safe_str(rid) if rid is not None else f\"ragtruth_{idx}\"\n",
    "\n",
    "    # Question/query/prompt\n",
    "    question = pick_first_present(row, (\"question\", \"query\", \"prompt\", \"instruction\"))\n",
    "    if question is None:\n",
    "        question = pick_first_present(row, (\"input\", \"source\", \"document\"))\n",
    "    question = safe_str(question).strip() if question is not None else \"[MISSING_QUESTION]\"\n",
    "\n",
    "    # Context/evidence\n",
    "    context = pick_first_present(row, (\"context\", \"contexts\", \"retrieved_context\", \"evidence\", \"passages\"))\n",
    "    context = join_context(context)\n",
    "\n",
    "    # Answer/response/generation\n",
    "    answer = pick_first_present(row, (\"answer\", \"output\", \"response\", \"generation\", \"model_output\"))\n",
    "    answer = safe_str(answer).strip() if answer is not None else \"\"\n",
    "\n",
    "    # Task/source dataset (optional)\n",
    "    task = pick_first_present(row, (\"task\", \"task_type\", \"dataset\", \"source_dataset\", \"benchmark\"))\n",
    "    task = safe_str(task).strip() if task is not None else None\n",
    "\n",
    "    # Label (optional; varies across dataset variants)\n",
    "    label = pick_first_present(row, (\"label\", \"hallucination_labels_processed\", \"human_label\", \"faithfulness\", \"groundedness\", \"is_hallucination\"))\n",
    "    label = safe_str(label).strip() if label is not None else None\n",
    "\n",
    "    normalized = {\n",
    "        \"example_id\": example_id,\n",
    "        \"task\": task,\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer,\n",
    "        \"label\": label,\n",
    "        \"meta\": row,  # keep original row for traceability/debugging\n",
    "    }\n",
    "    return normalized\n",
    "\n",
    "print(\"Normalization helpers defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "708344e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Normalized example 0 ---\n",
      "example_id: 0\n",
      "task: Summary\n",
      "question: Summarize the following news within 116 words:\n",
      "context preview: Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her supposed death on March 31, 1945, the Bergen-Belsen concentration camp where she ha...\n",
      "answer preview: The Anne Frank House has revealed that Anne Frank and her older sister, Margot, likely died at least a month earlier than previously believed. The sisters, who were imprisoned in Nazi concentration ca...\n",
      "label: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "\n",
      "--- Normalized example 1 ---\n",
      "example_id: 1\n",
      "task: Summary\n",
      "question: Summarize the following news within 116 words:\n",
      "context preview: Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her supposed death on March 31, 1945, the Bergen-Belsen concentration camp where she ha...\n",
      "answer preview: New research released by the Anne Frank House reveals that Anne Frank and her older sister, Margot, died at least a month earlier than previously believed. The research involved re-examining archives ...\n",
      "label: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "\n",
      "--- Normalized example 2 ---\n",
      "example_id: 2\n",
      "task: Summary\n",
      "question: Summarize the following news within 116 words:\n",
      "context preview: Seventy years ago, Anne Frank died of typhus in a Nazi concentration camp at the age of 15. Just two weeks after her supposed death on March 31, 1945, the Bergen-Belsen concentration camp where she ha...\n",
      "answer preview: New research conducted by the Anne Frank House has revealed that Anne Frank and her sister Margot likely died in the Bergen-Belsen concentration camp at least a month earlier than previously believed....\n",
      "label: {'evident_conflict': 1, 'baseless_info': 1}\n",
      "Normalization preview looks OK.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Normalize a few rows and inspect\n",
    "\n",
    "preview = []\n",
    "for i in range(min(3, len(ds))):\n",
    "    preview.append(normalize_row(dict(ds[i]), i))\n",
    "\n",
    "for i, ex in enumerate(preview):\n",
    "    print(\"\\n--- Normalized example\", i, \"---\")\n",
    "    print(\"example_id:\", ex[\"example_id\"])\n",
    "    print(\"task:\", ex[\"task\"])\n",
    "    print(\"question:\", ex[\"question\"][:200] + (\"...\" if len(ex[\"question\"]) > 200 else \"\"))\n",
    "    print(\"context preview:\", ex[\"context\"][:200] + (\"...\" if len(ex[\"context\"]) > 200 else \"\"))\n",
    "    print(\"answer preview:\", ex[\"answer\"][:200] + (\"...\" if len(ex[\"answer\"]) > 200 else \"\"))\n",
    "    print(\"label:\", ex[\"label\"])\n",
    "\n",
    "assert all(\"question\" in ex and \"context\" in ex and \"answer\" in ex for ex in preview)\n",
    "print(\"Normalization preview looks OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70b2962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing rows: 100%|██████████| 15090/15090 [00:01<00:00, 10276.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed JSONL: ../data/processed/ragtruth_processed.jsonl (15090 rows)\n",
      "Saved sample JSONL: ../data/samples/ragtruth_sample.jsonl (50 rows)\n"
     ]
    }
   ],
   "source": [
    "# Build processed dataset and write JSONL outputs\n",
    "\n",
    "processed_rows = []\n",
    "sample_rows = []\n",
    "\n",
    "n_total = len(ds) if LIMIT is None else min(LIMIT, len(ds))\n",
    "for i in tqdm(range(n_total), desc=\"Normalizing rows\"):\n",
    "    row = dict(ds[i])\n",
    "    norm = normalize_row(row, i)\n",
    "    processed_rows.append(norm)\n",
    "    if len(sample_rows) < SAMPLE_N:\n",
    "        sample_rows.append(norm)\n",
    "\n",
    "write_jsonl(PROCESSED_OUT, processed_rows)\n",
    "write_jsonl(SAMPLE_OUT, sample_rows)\n",
    "\n",
    "print(f\"Saved processed JSONL: {PROCESSED_OUT} ({len(processed_rows)} rows)\")\n",
    "print(f\"Saved sample JSONL: {SAMPLE_OUT} ({len(sample_rows)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5953c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read back 3 rows from processed file.\n",
      "Row 0 missing keys: set()\n",
      "Row 1 missing keys: set()\n",
      "Row 2 missing keys: set()\n",
      "Processed schema looks consistent.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Read back processed file and validate schema\n",
    "\n",
    "assert PROCESSED_OUT.exists()\n",
    "assert SAMPLE_OUT.exists()\n",
    "\n",
    "def read_first_n_jsonl(path: Path, n=3):\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for _ in range(n):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "rows_back = read_first_n_jsonl(PROCESSED_OUT, n=3)\n",
    "print(\"Read back\", len(rows_back), \"rows from processed file.\")\n",
    "\n",
    "required_keys = {\"example_id\", \"question\", \"context\", \"answer\", \"label\", \"meta\", \"task\"}\n",
    "for i, r in enumerate(rows_back):\n",
    "    missing = required_keys - set(r.keys())\n",
    "    print(f\"Row {i} missing keys:\", missing)\n",
    "    assert not missing\n",
    "\n",
    "print(\"Processed schema looks consistent.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
