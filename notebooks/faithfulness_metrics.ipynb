{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968ba0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge results path: ../results/runs/rag_judge_faithfulness/phase2_judge_outputs_sample.jsonl\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "# setup - imports and paths\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "JUDGE_RESULTS_PATH = Path(\"..\") / \"results\" / \"runs\"/ \"rag_judge_faithfulness\"/ \"phase2_judge_outputs_sample.jsonl\"\n",
    "\n",
    "print(\"Judge results path:\", JUDGE_RESULTS_PATH)\n",
    "print(\"Exists:\", JUDGE_RESULTS_PATH.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1567fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 judge outputs found. Proceeding to metrics.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Judge output file availability\n",
    "\n",
    "assert JUDGE_RESULTS_PATH.exists(), \"Phase 2 judge outputs not found.\"\n",
    "print(\"Phase 2 judge outputs found. Proceeding to metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916becf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 6\n",
      "Keys: ['example_id', 'judge_model', 'judge_label', 'judge_confidence', 'judge_evidence', 'judge_notes', 'raw_judge_output', 'human_label']\n"
     ]
    }
   ],
   "source": [
    "#Load judge outputs\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "rows = read_jsonl(JUDGE_RESULTS_PATH)\n",
    "\n",
    "print(\"Loaded rows:\", len(rows))\n",
    "print(\"Keys:\", list(rows[0].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb050ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required fields present.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Required fields present\n",
    "\n",
    "required = [\n",
    "    \"example_id\",\n",
    "    \"judge_label\",\n",
    "    \"judge_confidence\",\n",
    "    \"human_label\",\n",
    "]\n",
    "\n",
    "for r in rows:\n",
    "    for k in required:\n",
    "        assert k in r, f\"Missing key {k}\"\n",
    "\n",
    "print(\"All required fields present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef63678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge label distribution:\n",
      "Counter({'supported': 5, 'partially_supported': 1})\n",
      "\n",
      "Human label distribution (raw, as-is from dataset):\n",
      "Counter({\"{'evident_conflict': 0, 'baseless_info': 0}\": 5, \"{'evident_conflict': 1, 'baseless_info': 1}\": 1})\n"
     ]
    }
   ],
   "source": [
    "# Inspect raw label distributions\n",
    "\n",
    "judge_labels = [r[\"judge_label\"] for r in rows]\n",
    "human_labels = [r[\"human_label\"] for r in rows if r[\"human_label\"] is not None]\n",
    "\n",
    "print(\"Judge label distribution:\")\n",
    "print(Counter(judge_labels))\n",
    "\n",
    "print(\"\\nHuman label distribution (raw, as-is from dataset):\")\n",
    "print(Counter(human_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b2f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both judge and human labels are non-empty.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Non-empty labels\n",
    "\n",
    "assert len(judge_labels) > 0\n",
    "assert len(human_labels) > 0\n",
    "print(\"Both judge and human labels are non-empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b3224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize labels into binary faithfulness classes\n",
    "\n",
    "def normalize_human_label(label):\n",
    "    \"\"\"\n",
    "    Normalize RAGTruth human annotations into binary faithfulness.\n",
    "\n",
    "    Rule:\n",
    "    - faithful        if evident_conflict == 0 AND baseless_info == 0\n",
    "    - hallucinated    otherwise\n",
    "    \"\"\"\n",
    "    if label is None:\n",
    "        return None\n",
    "\n",
    "    # Case 1: label is already a dict\n",
    "    if isinstance(label, dict):\n",
    "        evident_conflict = label.get(\"evident_conflict\")\n",
    "        baseless_info = label.get(\"baseless_info\")\n",
    "\n",
    "    # Case 2: label is a stringified dict (common in JSONL)\n",
    "    elif isinstance(label, str):\n",
    "        try:\n",
    "            parsed = eval(label)\n",
    "            if not isinstance(parsed, dict):\n",
    "                return None\n",
    "            evident_conflict = parsed.get(\"evident_conflict\")\n",
    "            baseless_info = parsed.get(\"baseless_info\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Apply RAGTruth rule\n",
    "    if evident_conflict == 0 and baseless_info == 0:\n",
    "        return \"faithful\"\n",
    "    else:\n",
    "        return \"hallucinated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12b60a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = []\n",
    "\n",
    "for r in rows:\n",
    "    h = normalize_human_label(r[\"human_label\"])\n",
    "    if h is None:\n",
    "        continue\n",
    "\n",
    "    normalized.append({\n",
    "        \"judge\": normalize_judge_label(r[\"judge_label\"]),\n",
    "        \"human\": h,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8461d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels seen: {'hallucinated', 'faithful'}\n",
      "Normalized labels look correct.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Normalized labels sanity\n",
    "\n",
    "labels_seen = set()\n",
    "for r in normalized:\n",
    "    labels_seen.add(r[\"judge\"])\n",
    "    labels_seen.add(r[\"human\"])\n",
    "\n",
    "print(\"Labels seen:\", labels_seen)\n",
    "assert labels_seen.issubset({\"faithful\", \"hallucinated\"})\n",
    "print(\"Normalized labels look correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "673ef677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "TP: 0\n",
      "TN: 4\n",
      "FP: 1\n",
      "FN: 1\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix computation\n",
    "\n",
    "confusion = {\n",
    "    \"TP\": 0,  # hallucinated correctly detected\n",
    "    \"TN\": 0,  # faithful correctly detected\n",
    "    \"FP\": 0,  # faithful predicted hallucinated\n",
    "    \"FN\": 0,  # hallucinated predicted faithful\n",
    "}\n",
    "\n",
    "for r in normalized:\n",
    "    if r[\"human\"] == \"hallucinated\" and r[\"judge\"] == \"hallucinated\":\n",
    "        confusion[\"TP\"] += 1\n",
    "    elif r[\"human\"] == \"faithful\" and r[\"judge\"] == \"faithful\":\n",
    "        confusion[\"TN\"] += 1\n",
    "    elif r[\"human\"] == \"faithful\" and r[\"judge\"] == \"hallucinated\":\n",
    "        confusion[\"FP\"] += 1\n",
    "    elif r[\"human\"] == \"hallucinated\" and r[\"judge\"] == \"faithful\":\n",
    "        confusion[\"FN\"] += 1\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "for k, v in confusion.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2875405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Index: 0\n",
      "example_id (row): 0\n",
      "judge_label: supported\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 1\n",
      "example_id (row): 1\n",
      "judge_label: partially_supported\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: hallucinated\n",
      "------------------------------------------------------------\n",
      "Index: 2\n",
      "example_id (row): 2\n",
      "judge_label: supported\n",
      "human_label_raw: {'evident_conflict': 1, 'baseless_info': 1}\n",
      "normalized_human: hallucinated\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 3\n",
      "example_id (row): 6\n",
      "judge_label: supported\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 4\n",
      "example_id (row): 7\n",
      "judge_label: supported\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 5\n",
      "example_id (row): 8\n",
      "judge_label: supported\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n"
     ]
    }
   ],
   "source": [
    "# Checking each row corresponding decision\n",
    "for i, (r, n) in enumerate(zip(rows, normalized)):\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Index:\", i)\n",
    "    print(\"example_id (row):\", r[\"example_id\"])\n",
    "    print(\"judge_label:\", r[\"judge_label\"])\n",
    "    print(\"human_label_raw:\", r[\"human_label\"])\n",
    "    print(\"normalized_human:\", n[\"human\"])\n",
    "    print(\"normalized_judge:\", n[\"judge\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45acf163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 6\n",
      "Confusion matrix totals match.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Confusion matrix totals\n",
    "\n",
    "total = sum(confusion.values())\n",
    "print(\"Total evaluated rows:\", total)\n",
    "assert total == len(normalized)\n",
    "print(\"Confusion matrix totals match.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a0bc708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics (hallucination detection):\n",
      "Accuracy : 0.667\n",
      "Precision: 0.000\n",
      "Recall   : 0.000\n",
      "F1-score : 0.000\n"
     ]
    }
   ],
   "source": [
    "# Metric calculations (accuracy, precision, recall, F1)\n",
    "\n",
    "TP = confusion[\"TP\"]\n",
    "TN = confusion[\"TN\"]\n",
    "FP = confusion[\"FP\"]\n",
    "FN = confusion[\"FN\"]\n",
    "\n",
    "accuracy = (TP + TN) / max(1, TP + TN + FP + FN)\n",
    "precision = TP / max(1, TP + FP)\n",
    "recall = TP / max(1, TP + FN)\n",
    "f1 = (2 * precision * recall) / max(1e-8, precision + recall)\n",
    "\n",
    "print(\"Metrics (hallucination detection):\")\n",
    "print(f\"Accuracy : {accuracy:.3f}\") # Out of all answers, how often did the judge get it right\n",
    "print(f\"Precision: {precision:.3f}\") # When the judge says hallucinated, how often is it actually hallucinated\n",
    "print(f\"Recall   : {recall:.3f}\") # Out of all real hallucinations, how many did the judge catch\n",
    "print(f\"F1-score : {f1:.3f}\") # A single number that balances precision and recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94226f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics within expected bounds.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check Cell 12: Metric bounds\n",
    "\n",
    "for name, val in {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "}.items():\n",
    "    assert 0.0 <= val <= 1.0, f\"{name} out of bounds\"\n",
    "\n",
    "print(\"All metrics within expected bounds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec278f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg confidence (correct): 0.8\n",
      "Avg confidence (incorrect): 0.98\n",
      "Count correct: 1\n",
      "Count incorrect: 5\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Confidence analysis (judge confidence vs correctness)\n",
    "\n",
    "conf_correct = []\n",
    "conf_incorrect = []\n",
    "\n",
    "for r, n in zip(rows, normalized):\n",
    "    is_correct = (n[\"judge\"] == n[\"human\"])\n",
    "    if is_correct:\n",
    "        conf_correct.append(r[\"judge_confidence\"])\n",
    "    else:\n",
    "        conf_incorrect.append(r[\"judge_confidence\"])\n",
    "\n",
    "print(\"Avg confidence (correct):\", round(float(np.mean(conf_correct)), 3))\n",
    "print(\"Avg confidence (incorrect):\", round(float(np.mean(conf_incorrect)), 3))\n",
    "print(\"Count correct:\", len(conf_correct))\n",
    "print(\"Count incorrect:\", len(conf_incorrect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c01dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence analysis completed.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check Cell 14: Confidence arrays non-empty\n",
    "\n",
    "assert len(conf_correct) > 0\n",
    "assert len(conf_incorrect) > 0\n",
    "print(\"Confidence analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cefd4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives (missed hallucinations): 5\n",
      "False positives (over-flagged): 0\n",
      "\n",
      " False Negatives\n",
      "--------------------------------------------------------------------------------\n",
      "example_id: 0\n",
      "judge_label: supported\n",
      "human_label: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "notes: \n",
      "answer preview: {\n",
      "  \"label\": \"supported\",\n",
      "  \"confidence\": 1.0,\n",
      "  \"evidence\": \"The Anne Frank House has revealed that Anne Frank and her older sister, Margot, likely died at least a month earlier than previously believed.\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "--------------------------------------------------------------------------------\n",
      "example_id: 2\n",
      "judge_label: supported\n",
      "human_label: {'evident_conflict': 1, 'baseless_info': 1}\n",
      "notes: \n",
      "answer preview: {\n",
      "  \"label\": \"supported\",\n",
      "  \"confidence\": 1.0,\n",
      "  \"evidence\": \"New research conducted by the Anne Frank House has revealed that Anne Frank and her sister Margot likely died in the Bergen-Belsen concentration camp at least a month earlier than previously believed.\",\n",
      "  \"notes\": \"\"\n",
      "}\n",
      "\n",
      " False Positives\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Error analysis - show a few false negatives and false positives\n",
    "\n",
    "false_negatives = []\n",
    "false_positives = []\n",
    "\n",
    "for r, n in zip(rows, normalized):\n",
    "    if n[\"human\"] == \"hallucinated\" and n[\"judge\"] == \"faithful\":\n",
    "        false_negatives.append(r)\n",
    "    elif n[\"human\"] == \"faithful\" and n[\"judge\"] == \"hallucinated\":\n",
    "        false_positives.append(r)\n",
    "\n",
    "print(\"False negatives (missed hallucinations):\", len(false_negatives))\n",
    "print(\"False positives (over-flagged):\", len(false_positives))\n",
    "\n",
    "def preview_errors(errs, title, k=2):\n",
    "    print(\"\\n\", title)\n",
    "    for e in errs[:k]:\n",
    "        print(\"-\" * 80)\n",
    "        print(\"example_id:\", e[\"example_id\"])\n",
    "        print(\"judge_label:\", e[\"judge_label\"])\n",
    "        print(\"human_label:\", e[\"human_label\"])\n",
    "        print(\"notes:\", e[\"judge_notes\"])\n",
    "        print(\"answer preview:\", e[\"raw_judge_output\"][:300])\n",
    "\n",
    "preview_errors(false_negatives, \"False Negatives\")\n",
    "preview_errors(false_positives, \"False Positives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "429ef692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed quantitative metrics and qualitative error analysis.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check Cell 16: Phase 3 completion check\n",
    "\n",
    "print(\"Computed quantitative metrics and qualitative error analysis.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
