{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "968ba0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge results path: ../results/runs/rag_judge_faithfulness/phase2_judge_outputs_sample.jsonl\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "# setup - imports and paths\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "JUDGE_RESULTS_PATH = Path(\"..\") / \"results\" / \"runs\"/ \"rag_judge_faithfulness\"/ \"phase2_judge_outputs_sample.jsonl\"\n",
    "\n",
    "print(\"Judge results path:\", JUDGE_RESULTS_PATH)\n",
    "print(\"Exists:\", JUDGE_RESULTS_PATH.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1567fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judge outputs found. Proceeding to metrics.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Judge output file availability\n",
    "\n",
    "assert JUDGE_RESULTS_PATH.exists(), \"Phase 2 judge outputs not found.\"\n",
    "print(\"judge outputs found. Proceeding to metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "916becf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 10\n",
      "Keys in one row: ['example_id', 'judge_model', 'evaluation', 'nll', 'reason', 'raw_judge_output', 'human_label']\n"
     ]
    }
   ],
   "source": [
    "# Load judge outputs\n",
    "\n",
    "def read_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "rows = read_jsonl(JUDGE_RESULTS_PATH)\n",
    "\n",
    "print(\"Loaded rows:\", len(rows))\n",
    "print(\"Keys in one row:\", list(rows[0].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccb050ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required fields present.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Required fields present (PASS / FAIL judge)\n",
    "\n",
    "required = [\n",
    "    \"example_id\",\n",
    "    \"evaluation\",     # PASS / FAIL\n",
    "    \"nll\",            # uncertainty signal\n",
    "    \"reason\",         # short explanation\n",
    "    \"human_label\",\n",
    "]\n",
    "\n",
    "for r in rows:\n",
    "    for k in required:\n",
    "        assert k in r, f\"Missing key {k}\"\n",
    "\n",
    "print(\"All required fields present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cef63678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge label distribution:\n",
      "Counter({'PASS': 10})\n",
      "\n",
      "Human label distribution (raw, as-is from dataset):\n",
      "Counter({\"{'evident_conflict': 0, 'baseless_info': 0}\": 7, \"{'evident_conflict': 1, 'baseless_info': 1}\": 2, \"{'evident_conflict': 0, 'baseless_info': 1}\": 1})\n"
     ]
    }
   ],
   "source": [
    "# Inspect raw label distributions\n",
    "\n",
    "judge_labels = [r[\"evaluation\"] for r in rows]\n",
    "human_labels = [r[\"human_label\"] for r in rows if r[\"human_label\"] is not None]\n",
    "\n",
    "print(\"Judge label distribution:\")\n",
    "print(Counter(judge_labels))\n",
    "\n",
    "print(\"\\nHuman label distribution (raw, as-is from dataset):\")\n",
    "print(Counter(human_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1b2f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both judge evaluations and human labels are non-empty.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Non-empty judge evaluations and human labels\n",
    "\n",
    "assert len(judge_labels) > 0, \"No judge evaluations found.\"\n",
    "assert len(human_labels) > 0, \"No human labels found.\"\n",
    "\n",
    "print(\"Both judge evaluations and human labels are non-empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b3224c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize labels into binary faithfulness classes\n",
    "\n",
    "def normalize_human_label(label):\n",
    "    \"\"\"\n",
    "    Normalize RAGTruth human annotations into binary faithfulness.\n",
    "\n",
    "    Rule:\n",
    "    - faithful        if evident_conflict == 0 AND baseless_info == 0\n",
    "    - hallucinated    otherwise\n",
    "    \"\"\"\n",
    "    if label is None:\n",
    "        return None\n",
    "\n",
    "    # Case 1: label is already a dict\n",
    "    if isinstance(label, dict):\n",
    "        evident_conflict = label.get(\"evident_conflict\")\n",
    "        baseless_info = label.get(\"baseless_info\")\n",
    "\n",
    "    # Case 2: label is a stringified dict (common in JSONL)\n",
    "    elif isinstance(label, str):\n",
    "        try:\n",
    "            parsed = eval(label)\n",
    "            if not isinstance(parsed, dict):\n",
    "                return None\n",
    "            evident_conflict = parsed.get(\"evident_conflict\")\n",
    "            baseless_info = parsed.get(\"baseless_info\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # Apply RAGTruth rule\n",
    "    if evident_conflict == 0 and baseless_info == 0:\n",
    "        return \"faithful\"\n",
    "    else:\n",
    "        return \"hallucinated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e56bf495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_judge_evaluation(evaluation):\n",
    "    \"\"\"\n",
    "    Normalize judge evaluation into binary faithfulness.\n",
    "\n",
    "    Rule:\n",
    "    - PASS -> faithful\n",
    "    - FAIL -> hallucinated\n",
    "    \"\"\"\n",
    "    if evaluation == \"PASS\":\n",
    "        return \"faithful\"\n",
    "    elif evaluation == \"FAIL\":\n",
    "        return \"hallucinated\"\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12b60a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = []\n",
    "\n",
    "for r in rows:\n",
    "    human_norm = normalize_human_label(r[\"human_label\"])\n",
    "    if human_norm is None:\n",
    "        continue\n",
    "\n",
    "    judge_norm = normalize_judge_evaluation(r[\"evaluation\"])\n",
    "    if judge_norm is None:\n",
    "        continue\n",
    "\n",
    "    normalized.append({\n",
    "        \"judge\": judge_norm,\n",
    "        \"human\": human_norm,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8461d025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels seen: {'hallucinated', 'faithful'}\n",
      "Normalized labels look correct.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Normalized labels sanity\n",
    "\n",
    "labels_seen = set()\n",
    "for r in normalized:\n",
    "    labels_seen.add(r[\"judge\"])\n",
    "    labels_seen.add(r[\"human\"])\n",
    "\n",
    "print(\"Labels seen:\", labels_seen)\n",
    "assert labels_seen.issubset({\"faithful\", \"hallucinated\"})\n",
    "print(\"Normalized labels look correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "673ef677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "TP: 0\n",
      "TN: 7\n",
      "FP: 0\n",
      "FN: 3\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix computation\n",
    "\n",
    "confusion = {\n",
    "    \"TP\": 0,  # hallucinated correctly detected\n",
    "    \"TN\": 0,  # faithful correctly detected\n",
    "    \"FP\": 0,  # faithful predicted hallucinated\n",
    "    \"FN\": 0,  # hallucinated predicted faithful\n",
    "}\n",
    "\n",
    "for r in normalized:\n",
    "    if r[\"human\"] == \"hallucinated\" and r[\"judge\"] == \"hallucinated\":\n",
    "        confusion[\"TP\"] += 1\n",
    "    elif r[\"human\"] == \"faithful\" and r[\"judge\"] == \"faithful\":\n",
    "        confusion[\"TN\"] += 1\n",
    "    elif r[\"human\"] == \"faithful\" and r[\"judge\"] == \"hallucinated\":\n",
    "        confusion[\"FP\"] += 1\n",
    "    elif r[\"human\"] == \"hallucinated\" and r[\"judge\"] == \"faithful\":\n",
    "        confusion[\"FN\"] += 1\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "for k, v in confusion.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2875405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Index: 0\n",
      "example_id (row): 0\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 1\n",
      "example_id (row): 1\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 2\n",
      "example_id (row): 2\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 1, 'baseless_info': 1}\n",
      "normalized_human: hallucinated\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 3\n",
      "example_id (row): 3\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 1}\n",
      "normalized_human: hallucinated\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 4\n",
      "example_id (row): 4\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 5\n",
      "example_id (row): 5\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 1, 'baseless_info': 1}\n",
      "normalized_human: hallucinated\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 6\n",
      "example_id (row): 6\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 7\n",
      "example_id (row): 7\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 8\n",
      "example_id (row): 8\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n",
      "------------------------------------------------------------\n",
      "Index: 9\n",
      "example_id (row): 9\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 0}\n",
      "normalized_human: faithful\n",
      "normalized_judge: faithful\n"
     ]
    }
   ],
   "source": [
    "# Checking each row corresponding decision\n",
    "\n",
    "for i, (r, n) in enumerate(zip(rows, normalized)):\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Index:\", i)\n",
    "    print(\"example_id (row):\", r[\"example_id\"])\n",
    "    print(\"judge_evaluation:\", r[\"evaluation\"])\n",
    "    print(\"human_label_raw:\", r[\"human_label\"])\n",
    "    print(\"normalized_human:\", n[\"human\"])\n",
    "    print(\"normalized_judge:\", n[\"judge\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45acf163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total evaluated rows: 10\n",
      "Confusion matrix totals match.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check: Confusion matrix totals\n",
    "\n",
    "total = sum(confusion.values())\n",
    "print(\"Total evaluated rows:\", total)\n",
    "assert total == len(normalized)\n",
    "print(\"Confusion matrix totals match.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a0bc708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics (hallucination detection):\n",
      "Accuracy : 0.700\n",
      "Precision: 0.000\n",
      "Recall   : 0.000\n",
      "F1-score : 0.000\n"
     ]
    }
   ],
   "source": [
    "# Metric calculations (accuracy, precision, recall, F1)\n",
    "\n",
    "TP = confusion[\"TP\"]\n",
    "TN = confusion[\"TN\"]\n",
    "FP = confusion[\"FP\"]\n",
    "FN = confusion[\"FN\"]\n",
    "\n",
    "accuracy = (TP + TN) / max(1, TP + TN + FP + FN)\n",
    "precision = TP / max(1, TP + FP)\n",
    "recall = TP / max(1, TP + FN)\n",
    "f1 = (2 * precision * recall) / max(1e-8, precision + recall)\n",
    "\n",
    "print(\"Metrics (hallucination detection):\")\n",
    "print(f\"Accuracy : {accuracy:.3f}\") # Out of all answers, how often did the judge get it right\n",
    "print(f\"Precision: {precision:.3f}\") # When the judge says hallucinated, how often is it actually hallucinated\n",
    "print(f\"Recall   : {recall:.3f}\") # Out of all real hallucinations, how many did the judge catch\n",
    "print(f\"F1-score : {f1:.3f}\") # A single number that balances precision and recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b94226f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics within expected bounds.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check Cell 12: Metric bounds\n",
    "\n",
    "for name, val in {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "}.items():\n",
    "    assert 0.0 <= val <= 1.0, f\"{name} out of bounds\"\n",
    "\n",
    "print(\"All metrics within expected bounds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec278f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg NLL (correct predictions): 0.043\n",
      "Avg NLL (incorrect predictions): 0.06\n",
      "Count correct: 7\n",
      "Count incorrect: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Uncertainty analysis (NLL vs correctness)\n",
    "\n",
    "nll_correct = []\n",
    "nll_incorrect = []\n",
    "\n",
    "for r, n in zip(rows, normalized):\n",
    "    is_correct = (n[\"judge\"] == n[\"human\"])\n",
    "    if is_correct:\n",
    "        nll_correct.append(r[\"nll\"])\n",
    "    else:\n",
    "        nll_incorrect.append(r[\"nll\"])\n",
    "\n",
    "print(\"Avg NLL (correct predictions):\", round(float(np.mean(nll_correct)), 3))\n",
    "print(\"Avg NLL (incorrect predictions):\", round(float(np.mean(nll_incorrect)), 3))\n",
    "print(\"Count correct:\", len(nll_correct))\n",
    "print(\"Count incorrect:\", len(nll_incorrect))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18564dc",
   "metadata": {},
   "source": [
    "## Note: NLL (Uncertainty) Analysis Results\n",
    "\n",
    "Results:\n",
    "- Avg NLL (correct predictions): 0.043\n",
    "- Avg NLL (incorrect predictions): 0.060\n",
    "\n",
    "Lower NLL corresponds to higher confidence.  \n",
    "Even though the judge made incorrect decisions on hallucinated answers, those incorrect predictions had **higher NLL on average** than correct ones.\n",
    "\n",
    "This indicates that:\n",
    "- The judge is often *confidently accepting* answers,\n",
    "- But it is **less confident when it is wrong**.\n",
    "\n",
    "Although the decision boundary (PASS / FAIL) is currently weak, the uncertainty signal itself is informative.\n",
    "This suggests that NLL can be leveraged for:\n",
    "- threshold-based filtering,\n",
    "- risk-aware evaluation,\n",
    "- or prioritizing low-confidence answers for review.\n",
    "\n",
    "In short, the judge is poorly calibrated in classification, but its uncertainty estimates already contain useful signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73c01dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL uncertainty analysis completed.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check Cell 14: NLL arrays non-empty\n",
    "\n",
    "assert len(nll_correct) > 0, \"No correct predictions found for NLL analysis.\"\n",
    "assert len(nll_incorrect) > 0, \"No incorrect predictions found for NLL analysis.\"\n",
    "\n",
    "print(\"NLL uncertainty analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cefd4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives (missed hallucinations): 3\n",
      "False positives (over-flagged): 0\n",
      "\n",
      " False Negatives\n",
      "--------------------------------------------------------------------------------\n",
      "example_id: 2\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 1, 'baseless_info': 1}\n",
      "reason: The answer restates facts that are explicitly mentioned in the provided context, including the new research findings, the conditions at Bergen-Belsen concentration camp, and the likely dates of death for Anne Frank and her sister Margot.\n",
      "nll: 0.01\n",
      "raw_judge_output preview:\n",
      "EVALUATION: PASS\n",
      "REASON: The answer restates facts that are explicitly mentioned in the provided context, including the new research findings, the conditions at Bergen-Belsen concentration camp, and the likely dates of death for Anne Frank and her sister Margot.\n",
      "NLL: 0.01\n",
      "--------------------------------------------------------------------------------\n",
      "example_id: 3\n",
      "judge_evaluation: PASS\n",
      "human_label_raw: {'evident_conflict': 0, 'baseless_info': 1}\n",
      "reason: The answer restates facts that are explicitly mentioned in the provided context.\n",
      "nll: 0.12\n",
      "raw_judge_output preview:\n",
      "EVALUATION: PASS\n",
      "REASON: The answer restates facts that are explicitly mentioned in the provided context.\n",
      "NLL: 0.12\n",
      "\n",
      " False Positives\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Error analysis - show a few false negatives and false positives\n",
    "\n",
    "false_negatives = []\n",
    "false_positives = []\n",
    "\n",
    "for r, n in zip(rows, normalized):\n",
    "    if n[\"human\"] == \"hallucinated\" and n[\"judge\"] == \"faithful\":\n",
    "        false_negatives.append(r)\n",
    "    elif n[\"human\"] == \"faithful\" and n[\"judge\"] == \"hallucinated\":\n",
    "        false_positives.append(r)\n",
    "\n",
    "print(\"False negatives (missed hallucinations):\", len(false_negatives))\n",
    "print(\"False positives (over-flagged):\", len(false_positives))\n",
    "\n",
    "def preview_errors(errs, title, k=2):\n",
    "    print(\"\\n\", title)\n",
    "    for e in errs[:k]:\n",
    "        print(\"-\" * 80)\n",
    "        print(\"example_id:\", e[\"example_id\"])\n",
    "        print(\"judge_evaluation:\", e[\"evaluation\"])\n",
    "        print(\"human_label_raw:\", e[\"human_label\"])\n",
    "        print(\"reason:\", e[\"reason\"])\n",
    "        print(\"nll:\", e[\"nll\"])\n",
    "        print(\"raw_judge_output preview:\")\n",
    "        print(e[\"raw_judge_output\"][:300])\n",
    "\n",
    "preview_errors(false_negatives, \"False Negatives\")\n",
    "preview_errors(false_positives, \"False Positives\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "429ef692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed quantitative metrics and qualitative error analysis.\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check Cell 16: Phase 3 completion check\n",
    "\n",
    "print(\"Computed quantitative metrics and qualitative error analysis.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
